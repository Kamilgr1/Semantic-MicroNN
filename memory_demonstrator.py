"""
ĞšĞ¾Ğ´ 1: ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¡ÑƒĞ±Ğ»Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ—Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ
================================================
Ğ˜Ğ»Ğ»ÑÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸: Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞºĞ¾Ñ€Ñ ĞºĞ°Ğº Ñ‚ĞµÑ€Ğ¼Ğ¾ÑÑ‚Ğ°Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.

Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚Ñ€Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°:
  BASELINE  â€” Ğ±ĞµĞ· ÑĞºĞ¾Ñ€Ñ (catastrophic forgetting)
  REPLAY    â€” ÑĞ²Ğ½Ñ‹Ğ¹ replay ÑÑ‚Ğ°Ñ€Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´)
  SUBLIMINAL â€” MSE-ÑĞºĞ¾Ñ€ÑŒ Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼ ÑˆÑƒĞ¼Ğµ (Ğ½Ğ°Ñˆ Ğ¼ĞµÑ‚Ğ¾Ğ´)

Ğ›Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚:
  - Retention Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ injection
  - Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ SR Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ injection
  - Ğ Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ Ğ² Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² (Ğ½Ğ¾Ñ€Ğ¼Ğ°, Ñ€Ğ°Ğ½Ğ³)

Ğ¢ĞµÑ€Ğ¼Ğ¾Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ:
  BASELINE   = ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±ĞµĞ· Ñ‚ĞµÑ€Ğ¼Ğ¾ÑÑ‚Ğ°Ñ‚Ğ° (T â†’ âˆ Ğ¿Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸)
  REPLAY     = Ñ‚ĞµÑ€Ğ¼Ğ¾ÑÑ‚Ğ°Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (ÑĞ²Ğ½Ğ¾Ğµ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ)
  SUBLIMINAL = Ñ‚ĞµÑ€Ğ¼Ğ¾ÑÑ‚Ğ°Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºĞ¾Ñ€ÑŒ (Ğ½ĞµÑĞ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ H_mem)
"""

import torch
import torch.nn as nn
import torch.optim as optim
import random
import copy
import numpy as np

# â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMBED_DIM    = 64
DOM_DIM      = 4
OP_DIM       = 6
DEVICE       = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE   = 64
ORTHO_LAMBDA = 0.05
SUBLIM_LAMBDA = 20.0
LR           = 0.002
STEPS_PRETRAIN = 10000   # Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾: Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞºĞ¾Ñ€Ñ Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹
STEPS_INJECT   = 6000
LOG_FREQ       = 1000
SEED         = 42

# â”€â”€ Ğ—ĞĞ”ĞĞ§Ğ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TaskGen:
    def __init__(self, domain, op):
        self.domain = domain
        self.op     = op

    def get(self, k=50):
        a, b = random.randint(0,k-1), random.randint(0,k-1)
        if   self.op == 0: res = (a+b) % k
        elif self.op == 1: res = abs(a-b)
        elif self.op == 2: res = max(a,b)
        elif self.op == 3: res = min(a,b)
        is_pos = random.random() > 0.5
        if not is_pos: res = (res + random.randint(1,k-1)) % k
        seq = [50+self.op, a, b, res, 76] if self.domain==0 \
              else [50+self.op, res, a, b, 76]
        return seq, float(is_pos)

def get_batch(gen, n):
    x,y = [],[]
    for _ in range(n):
        p,l = gen.get(); x.append(p); y.append(l)
    return (torch.LongTensor(x).to(DEVICE),
            torch.FloatTensor(y).unsqueeze(1).to(DEVICE))

# â”€â”€ ĞœĞĞ”Ğ•Ğ›Ğ¬ (Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¹ Ğ•Ğ²ĞºĞ»Ğ¸Ğ´ â€” Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾, Ğ±ĞµĞ· matrix_exp) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class EuclideanModel(nn.Module):
    """
    Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€: Ğ²ÑĞµ nn.Linear.
    ĞĞµÑ‚ matrix_exp â†’ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½.
    ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞºĞ¾Ñ€Ñ Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ â€” Ñ†ĞµĞ»ÑŒ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ
    Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ BASELINE / REPLAY / SUBLIMINAL, Ğ° Ğ½Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹.
    """
    def __init__(self):
        super().__init__()
        self.emb      = nn.Embedding(80, EMBED_DIM)
        self.proj_dom = nn.Linear(DOM_DIM, EMBED_DIM, bias=False)
        self.proj_op  = nn.Linear(OP_DIM,  EMBED_DIM, bias=False)
        self.q_proj   = nn.Linear(EMBED_DIM, EMBED_DIM, bias=False)
        self.k_proj   = nn.Linear(EMBED_DIM, EMBED_DIM, bias=False)
        self.v_proj   = nn.Linear(EMBED_DIM, EMBED_DIM, bias=False)
        self.lin1     = nn.Linear(EMBED_DIM, EMBED_DIM, bias=False)
        self.lin2     = nn.Linear(EMBED_DIM, EMBED_DIM, bias=False)
        self.head     = nn.Linear(EMBED_DIM, 1)

    def forward(self, x, kd, ko):
        h  = self.emb(x) + self.proj_dom(kd) + self.proj_op(ko)
        Q  = self.q_proj(h);  K = self.k_proj(h);  V = self.v_proj(h)
        at = torch.softmax((Q @ K.transpose(-2,-1))/(EMBED_DIM**.5), dim=-1)
        hm = h + at @ V
        ho = hm + self.lin2(torch.relu(self.lin1(hm)))
        return torch.sigmoid(self.head(ho.mean(1))), ho

    def stable_rank(self):
        with torch.no_grad():
            W = self.q_proj.weight
            S = torch.linalg.svdvals(W)
            return ((S**2).sum()/(S[0]**2)).item()

    def weight_norm(self):
        return self.q_proj.weight.norm().item()

def ortho_pen(m):
    return torch.norm(m.proj_dom.weight.t() @ m.proj_op.weight)

def evaluate(model, tasks, keys, names, n=500):
    model.eval()
    res = {}
    with torch.no_grad():
        for name in names:
            x,y = get_batch(tasks[name], n)
            kd,ko = keys[name]
            out,_ = model(x,kd,ko)
            res[name] = ((out>0.5).float()==y).float().mean().item()*100
    return res

# â”€â”€ Ğ­ĞšĞ¡ĞŸĞ•Ğ Ğ˜ĞœĞ•ĞĞ¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def setup(seed=SEED):
    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)

def build_tasks():
    setup()
    roots = [torch.zeros(DOM_DIM).to(DEVICE) for _ in range(2)]
    roots[0][0]=1.; roots[1][1]=1.
    deltas = [torch.zeros(OP_DIM).to(DEVICE) for _ in range(4)]
    for i in range(4): deltas[i][i]=1.
    keys, tasks = {}, {}
    for d in range(2):
        for o in range(4):
            n = f"D{d}_O{o}"
            keys[n]  = (roots[d].view(1,1,-1), deltas[o].view(1,1,-1))
            tasks[n] = TaskGen(d, o)
    return tasks, keys

def pretrain(model, tasks, keys, base_tasks):
    """ĞĞ±Ñ‰Ğ¸Ğ¹ pretraining Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ‚Ñ€Ñ‘Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² (Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¹)."""
    opt = optim.AdamW(model.parameters(), lr=LR)
    bce = nn.BCELoss()
    for step in range(1, STEPS_PRETRAIN+1):
        model.train(); opt.zero_grad()
        name = base_tasks[step % len(base_tasks)]  # round-robin
        x,y  = get_batch(tasks[name], BATCH_SIZE)
        kd,ko = keys[name]
        out,_ = model(x,kd,ko)
        loss  = bce(out,y) + ORTHO_LAMBDA * ortho_pen(model)
        loss.backward(); opt.step()
    return model

def inject_and_log(mode, model, tasks, keys,
                   base_tasks, new_task, retention_tasks):
    """
    Inject Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ‡ĞµÑ‚Ñ‹Ñ€ÑŒĞ¼Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ°Ğ¼Ğ¸.
    mode: 'baseline' | 'replay' | 'subliminal' | 'subliminal+freeze'

    subliminal+freeze:
      - Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ q/k/v_proj (ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ/routing)
      - Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ FFN + emb + head (ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ°)
      - MSE ÑĞºĞ¾Ñ€ÑŒ Ğ½Ğ° ÑˆÑƒĞ¼Ğµ Ğ´Ğ»Ñ Ğ½ĞµĞ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ñ‘Ğ²
      â†’ grok32-ÑÑ‚Ğ¸Ğ»ÑŒ, Ğ½Ğ¾ Ğ½Ğ° Euclidean Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ
    """
    bce    = nn.BCELoss()
    anchor = copy.deepcopy(model).eval() \
             if 'subliminal' in mode else None

    # â”€â”€ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ° Ğ´Ğ»Ñ subliminal+freeze â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if mode == 'subliminal+freeze':
        for name in ['q_proj', 'k_proj', 'v_proj']:
            getattr(model, name).requires_grad_(False)
        active = [p for p in model.parameters() if p.requires_grad]
    else:
        active = list(model.parameters())

    opt = optim.AdamW(active, lr=LR/2)

    print(f"\n  â”€â”€ MODE: {mode.upper()} â”€â”€")
    if mode == 'subliminal+freeze':
        print(f"     (q/k/v_proj Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ñ‹ â€” ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ñ‘Ğ½)")
    print(f"  {'Step':>6} | {'NewTask':>8} | "
          + " ".join(f"{t:>10}" for t in retention_tasks)
          + f" | {'SR':>6} | {'Norm':>6}")
    print(f"  {'-'*80}")

    log = []
    kd_new, ko_new = keys[new_task]

    for step in range(1, STEPS_INJECT+1):
        model.train(); opt.zero_grad()

        # â”€â”€ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº: Ğ½Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        x_new, y_new = get_batch(tasks[new_task], BATCH_SIZE)
        out_new, _   = model(x_new, kd_new, ko_new)
        loss_task    = bce(out_new, y_new)

        # â”€â”€ memory stream â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if mode == 'baseline':
            loss_mem   = torch.tensor(0.0).to(DEVICE)
            mem_lambda = 0.0

        elif mode == 'replay':
            past = base_tasks[step % len(base_tasks)]
            x_old, y_old = get_batch(tasks[past], BATCH_SIZE//2)
            kd_old, ko_old = keys[past]
            out_old, _ = model(x_old, kd_old, ko_old)
            loss_mem   = bce(out_old, y_old)
            mem_lambda = 1.0

        elif mode in ('subliminal', 'subliminal+freeze'):
            x_noise    = torch.randint(0, 77, (BATCH_SIZE, 5)).to(DEVICE)
            past       = base_tasks[step % len(base_tasks)]
            kd_p, ko_p = keys[past]
            with torch.no_grad():
                _, h_anch = anchor(x_noise, kd_p, ko_p)
            _, h_stud = model(x_noise, kd_p, ko_p)
            loss_mem   = nn.MSELoss()(h_stud, h_anch)
            mem_lambda = SUBLIM_LAMBDA

        loss = loss_task + mem_lambda * loss_mem + ORTHO_LAMBDA * ortho_pen(model)
        loss.backward(); opt.step()

        if step % LOG_FREQ == 0:
            model.eval()
            ret     = evaluate(model, tasks, keys, retention_tasks, n=300)
            sr      = model.stable_rank()
            nrm     = model.weight_norm()
            new_acc = evaluate(model, tasks, keys, [new_task], n=300)[new_task]
            model.train()

            ret_str = " ".join(f"{ret[t]:>9.1f}%" for t in retention_tasks)
            print(f"  {step:>6} | {new_acc:>7.1f}% | {ret_str} | {sr:>6.3f} | {nrm:>6.2f}")
            log.append({
                "step":      step,
                "new_acc":   new_acc,
                "retention": ret,
                "sr":        sr,
                "norm":      nrm,
                "loss_mem":  loss_mem.item() if hasattr(loss_mem, 'item') else 0,
            })

    # â”€â”€ Ñ€Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ° Ğ¿Ğ¾ÑĞ»Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if mode == 'subliminal+freeze':
        for name in ['q_proj', 'k_proj', 'v_proj']:
            getattr(model, name).requires_grad_(True)

    return log

def run():
    print(f"ğŸ§  ĞšĞĞ” 1: Ğ¡Ğ£Ğ‘Ğ›Ğ˜ĞœĞ˜ĞĞĞ›Ğ¬ĞĞĞ• Ğ—ĞĞŸĞĞœĞ˜ĞĞĞĞ˜Ğ•  |  device={DEVICE}")
    print(f"   Ğ¢ĞµÑ€Ğ¼Ğ¾Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ:")
    print(f"   BASELINE          = ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±ĞµĞ· Ñ‚ĞµÑ€Ğ¼Ğ¾ÑÑ‚Ğ°Ñ‚Ğ° (T â†’ âˆ)")
    print(f"   REPLAY            = Ñ‚ĞµÑ€Ğ¼Ğ¾ÑÑ‚Ğ°Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (ÑĞ²Ğ½Ğ¾Ğµ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ P)")
    print(f"   SUBLIMINAL        = ÑĞºĞ¾Ñ€ÑŒ Ğ½Ğ° ÑˆÑƒĞ¼Ğµ (Ğ¿Ğ¾Ğ»Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ H_mem)")
    print(f"   SUBLIMINAL+FREEZE = ÑĞºĞ¾Ñ€ÑŒ + Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ° ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸ÑĞ° (H_mem + V=const)")

    tasks, keys = build_tasks()
    base_tasks      = ["D0_O0","D0_O1","D0_O2","D1_O0","D1_O1","D1_O2"]
    new_task        = "D0_O3"
    retention_tasks = ["D0_O0","D0_O1","D1_O1","D1_O2"]

    # â”€â”€ Pretrain Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n{'='*80}")
    print(f"  STAGE 1: PRETRAIN (Ğ¾Ğ±Ñ‰Ğ¸Ğ¹, {STEPS_PRETRAIN} steps, round-robin)")
    print(f"{'='*80}")

    setup()
    base_model = EuclideanModel().to(DEVICE)
    base_model = pretrain(base_model, tasks, keys, base_tasks)

    ret_pre = evaluate(base_model, tasks, keys, retention_tasks)
    sr_pre  = base_model.stable_rank()
    print(f"\n  ĞŸĞ¾ÑĞ»Ğµ pretrain:")
    for t,v in ret_pre.items():
        bar = "â–ˆ"*int(v/5)
        print(f"    {t}: {v:.1f}%  {bar}")
    print(f"  SR: {sr_pre:.4f}")

    # â”€â”€ Stage 2: Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° injection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n{'='*80}")
    print(f"  STAGE 2: INJECTION Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ({new_task}), {STEPS_INJECT} steps")
    print(f"  Retention Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ {LOG_FREQ} ÑˆĞ°Ğ³Ğ¾Ğ²")
    print(f"{'='*80}")

    MODES = ['baseline', 'replay', 'subliminal', 'subliminal+freeze']
    all_logs = {}
    for mode in MODES:
        setup()
        model = copy.deepcopy(base_model)
        all_logs[mode] = inject_and_log(
            mode, model, tasks, keys,
            base_tasks, new_task, retention_tasks)

    # â”€â”€ Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    W = 14
    print(f"\n{'='*80}")
    print(f"  Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞĞ• Ğ¡Ğ ĞĞ’ĞĞ•ĞĞ˜Ğ•")
    print(f"  (ref: grok32 Riemannian+freeze â†’ Retentionâ‰ˆ88%, ZS=74.7%)")
    print(f"{'='*80}")
    header = f"  {'ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ°':<28}" + "".join(f" | {m.upper():>{W}}" for m in MODES)
    print(header)
    print(f"  {'-'*( 28 + (W+3)*len(MODES) )}")

    def final(mode, key):
        last = all_logs[mode][-1]
        if key == 'new_acc':  return last['new_acc']
        if key == 'sr':       return last['sr']
        if key == 'ret_avg':  return sum(last['retention'].values()) / len(last['retention'])
        return last['retention'].get(key, 0)

    metrics = [
        ("ĞĞ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°",        'new_acc',  "{:.1f}%"),
        ("Retention avg",       'ret_avg',  "{:.1f}%"),
        ("  D0_O0",             'D0_O0',    "{:.1f}%"),
        ("  D0_O1",             'D0_O1',    "{:.1f}%"),
        ("  D1_O1",             'D1_O1',    "{:.1f}%"),
        ("  D1_O2",             'D1_O2',    "{:.1f}%"),
        ("Stable Rank (final)", 'sr',       "{:.3f}"),
    ]
    for label, key, fmt in metrics:
        row = [fmt.format(final(m, key)) for m in MODES]
        print(f"  {label:<28}" + "".join(f" | {v:>{W}}" for v in row))

    # â”€â”€ SR Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n  SR Ğ¢Ğ ĞĞ•ĞšĞ¢ĞĞ Ğ˜Ğ˜ (Ñ‚ĞµÑ€Ğ¼Ğ¾ÑÑ‚Ğ°Ñ‚ Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸):")
    hdr = f"  {'Step':>6}" + "".join(f" | {m.upper():>{W}}" for m in MODES)
    print(hdr)
    print(f"  {'-'*(6 + (W+3)*len(MODES) + 2)}")
    n = len(all_logs['baseline'])
    for i in range(n):
        step = all_logs['baseline'][i]['step']
        srs  = [f"{all_logs[m][i]['sr']:>{W}.4f}" for m in MODES]
        print(f"  {step:>6}" + "".join(f" | {v}" for v in srs))

    # â”€â”€ Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n  Ğ¤Ğ˜Ğ—Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ˜ĞĞ¢Ğ•Ğ ĞŸĞ Ğ•Ğ¢ĞĞ¦Ğ˜Ğ¯:")
    print(f"  BASELINE:          SR Ğ´Ñ€ĞµĞ¹Ñ„ÑƒĞµÑ‚ Ğ²Ğ½Ğ¸Ğ· â†’ Ğ²ĞµÑĞ° Ğ¿ĞµÑ€ĞµÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾")
    print(f"  REPLAY:            SR Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½ĞµĞµ â†’ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ€Ğ¼Ğ¾Ğ·Ğ¸Ñ‚ Ğ´Ñ€ĞµĞ¹Ñ„")
    print(f"  SUBLIMINAL:        SR ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ĞµĞ½ â†’ H_mem ÑƒĞ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠÑ‘Ğ¼, Ğ½Ğ¾ Ğ½Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹")
    print(f"  SUBLIMINAL+FREEZE: SR+Retention â†’ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ° ÑĞºĞ¾Ñ€ĞµĞ½Ğ°")

    ret_sf = final('subliminal+freeze', 'ret_avg')
    ret_r  = final('replay', 'ret_avg')
    print(f"\n  Ğ’Ğ«Ğ’ĞĞ” Ğ”Ğ›Ğ¯ Ğ¡Ğ¢ĞĞ¢Ğ¬Ğ˜:")
    if ret_sf > ret_r:
        print(f"  âœ“ Subliminal+Freeze Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆÑ‘Ğ» Replay ({ret_sf:.1f}% > {ret_r:.1f}%)")
        print(f"    Ğ±ĞµĞ· ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ¸Ğ· ÑÑ‚Ğ°Ñ€Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡")
    elif ret_sf > final('subliminal', 'ret_avg'):
        gap = ret_sf - final('subliminal', 'ret_avg')
        print(f"  âœ“ Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ° ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸ÑĞ° Ğ´Ğ°Ñ‘Ñ‚ +{gap:.1f}% Ğº Retention")
        print(f"    Retention: Subliminal={final('subliminal','ret_avg'):.1f}% "
              f"â†’ Subliminal+Freeze={ret_sf:.1f}%")
    print(f"\nâœ… ĞšĞ¾Ğ´ 1 Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½.")


if __name__ == "__main__":
    run()